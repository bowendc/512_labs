{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5c6076a",
   "metadata": {},
   "source": [
    "# Spatial Analysis and Regression\n",
    "\n",
    "In this learning activity, we will be using a range of more advanced geoprocessing and spatial analysis tools. Buckle up, it will get a bit advanced!\n",
    "\n",
    "The main packages new packages we will use are **{spdep}** and **{spatialreg}** for working with our spatial weights matrices and running spatial models, respectively. \n",
    "\n",
    "One major issue in the United States is racial segregation; due to explicitly racist government policy like redlining, as well as more \"neutral\" policies single-family zoning and school district catchment zones, many American communities are segregated by race. In this activity, we will exam racial segregation in New Jersey by mapping and modeling racial *entropy* scores, a method for measuring diversity. We will rely on **{tidycensus}** to download both spatial and demographic data from the U.S. Census. Ready? Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81d1dda",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# if needed, install pacman\n",
    "install.packages('pacman')\n",
    "\n",
    "pacman::p_load(\n",
    "    tidyverse,      # tidy data management + graphing\n",
    "    sf,             # spatial data management and processing\n",
    "    tigris,         # downloading GIS files from Census\n",
    "    tidycensus,     # downloading census data + GIS\n",
    "    spdep,          # spatial weights and autocorrelation\n",
    "    spatialreg,     # spatial regression models\n",
    "    modelsummary    # creating regression tables\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a0da5a",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "Here we are going to use the `get_decennial()` function in order to get higher-quality data of very small geographic units, like census tracts. Note the use of the `sumfile` argument, and that we are getting more detailed geographic information by setting cartographic boundary files to off using `cb = FALSE`.\n",
    "\n",
    "We will use `vacancy.renter` as a our measure of poverty (the decennial census does not included poverty questions). Here I am assuming that vacant rental properties reflect high levels of undesired locations due to the problems of concentrated poverty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4ed9b4",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df <- get_decennial(state = \"NJ\",\n",
    "              year = 2020,\n",
    "              geography = \"tract\", \n",
    "              variables = c(median.age = \"DP1_0073C\",\n",
    "                            totalpop = \"DP1_0092C\",\n",
    "                            latino.pct = \"DP1_0093P\",\n",
    "                            white.pct = \"DP1_0105P\",\n",
    "                            black.pct = \"DP1_0106P\",\n",
    "                            nat.am.pct = \"DP1_0107P\",\n",
    "                            asian.pct = \"DP1_0108P\",\n",
    "                            vacancy.owner = \"DP1_0156C\",\n",
    "                            vacancy.renter = \"DP1_0157C\"),\n",
    "              geometry = TRUE,\n",
    "              sumfile = \"dp\",\n",
    "              cb = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069bbf9d",
   "metadata": {},
   "source": [
    "As is normal, we need to pivot our dataframe to move multiple variables into columns instead of rows. Let's also create a new variable which includes the rest of the racial/ethnic categories in NJ. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4f9804",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "df.wide <- df |> pivot_wider(names_from = variable,\n",
    "                             values_from = value) |>\n",
    "                 mutate(other.pct = 100 - (latino.pct + white.pct + \n",
    "                                           black.pct + nat.am.pct + \n",
    "                                           asian.pct))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eec1a6b",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "Some of our racial/ethnic variables have missing data, as the Census codes them as `NA` instead of missing. We can use a simple loop to recode these variables. Since we have the same syntax used over many variables, this is more efficient.\n",
    "\n",
    "Note that in order for us to create variable names inside the loop, we need to use some unique syntax inside of `mutate()`. The two !! points before `var.out` tells R that `var.out` will be evaluated; it isn't the name of the variable but is instead an object. Likewise, we can use the `.data[[]]` syntax to evaluate existing names we include through the `var.in` object. Lastly, in order for the function to work, we need to use `:=` instead of `=` in the formula.\n",
    "\n",
    "Entropy scores are created by measuring the proportion of observations in some category times the natural log of the inverse of the proportion. The formula belows does this and then codes the missing data as zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f757f9ff",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "for(i in c(\"latino\", \"white\", \"black\", \"nat.am\", \"asian\", \"other\")){\n",
    "  var.in <- paste0(i, \".pct\")\n",
    "  var.out <- paste0(\"entropy.\", i)\n",
    "  df.wide <- df.wide |> \n",
    "        mutate(!!var.out := (.data[[var.in]]/100)*log(1/(.data[[var.in]]/100)),\n",
    "                                !!var.out := ifelse(is.na(.data[[var.out]]), 0, .data[[var.out]]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2085a60",
   "metadata": {},
   "source": [
    "Now, add up all the entropy scores to create the final measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dca728",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df.wide <- df.wide |> \n",
    "        mutate(entropy = entropy.latino + \n",
    "        entropy.white + entropy.black + \n",
    "        entropy.nat.am + entropy.asian + entropy.other)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f3284c",
   "metadata": {},
   "source": [
    "Let's map it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e2c77d",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ggplot(df.wide, aes(fill = entropy)) + \n",
    "    geom_sf(linewidth = .01) + \n",
    "    scale_fill_viridis_c(option = \"A\") +\n",
    "    theme_void()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735f3ad1",
   "metadata": {},
   "source": [
    "There are definitely spatial patterns here. More populous areas seem to be more diverse, although there appears to be pockets of low diversity in some of New Jersey's city centers (Trenton, Newark, Patterson, etc). \n",
    "\n",
    "Before we do more, let's deal with the odd part of this map: tract shapes are clearly including parts of the ocean. We can remove that area using **{tigris}**'s `erase_water()` function. This can take several minutes to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d06707",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "df.wide <- df.wide |> \n",
    "            st_transform(32111) |>    # project to NAD / New Jersey \n",
    "            erase_water(year = 2020)  # removes water from polygon. \n",
    "                                      # year needs to match year from data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238b2920",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ggplot(df.wide, aes(fill = entropy)) + \n",
    "    geom_sf(linewidth = .01) + \n",
    "    scale_fill_viridis_c(option = \"A\") +\n",
    "    theme_void()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eefed9",
   "metadata": {},
   "source": [
    "Much better!\n",
    "\n",
    "Now, given the relationship between diversity and population density, let's create a formal measure of population density of the tract. We can use `st_area` from **{sf}** to measure the area. The function will use whatever unit is coded into your coordinate reference system. In this case, we'll be using squared meters. Below, we measure the area, and then divide the population of the tract by the area (converted to kilometers).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b94926c",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "df.wide$area = st_area(df.wide)\n",
    "df.wide$popden = df.wide$totalpop / (df.wide$area / 1000) # thousands of persons per sq kilometer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e4b9ad",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ggplot(df.wide, aes(fill = as.numeric(popden))) +     # because popden is measured in squared kilometers, we convert to numeric\n",
    "    geom_sf(linewidth = .01) + \n",
    "    scale_fill_viridis_c(option = \"A\", direction = -1) + # -1 reverses color scale\n",
    "    theme_void()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f73afb3",
   "metadata": {},
   "source": [
    "## Spatial Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628407ae",
   "metadata": {},
   "source": [
    "We can use a set of functions from **{spdep}** to create spatial weights matrices. Let's start with queen contiguity weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adee5a4",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# here poly2nb() takes our polygons and creates a list of all neighbors, defined as queen contiguity\n",
    "w_queen <- poly2nb(df.wide, queen = TRUE) \n",
    "# to use, we need to make them weights using nb2list2().\n",
    "w_q <- nb2listw(w_queen,         # from previous function\n",
    "                style = \"W\",     # W means row standardized\n",
    "                zero.policy = TRUE) # allows weights with no neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a29906",
   "metadata": {},
   "source": [
    "Creating distance or inverse distance weights is a little more complex. Below, we create two kinds. The first will include a list of all tracts within 20 kilometers of the tract. The second method will choose the 10 closest neighbors for each tract. In order for this to work, we need to use point data instead of polygon data. So we create polygon centroids using `st_centroid()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566ecaf7",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "coords <- st_centroid(df.wide)\n",
    "\n",
    "ggplot(coords) + geom_sf() + theme_void()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0895037b",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "w_dist <- dnearneigh(coords,   # identifies all neighbors inside of range\n",
    "            d1 = 0,            # minimum\n",
    "            d2 = 20000)        # maximum = 20km\n",
    "\n",
    "w_k <- knn2nb(knearneigh(coords, k = 10)) # creates neighbor list of the 20 nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31de3504",
   "metadata": {},
   "source": [
    "Now we use `nbdists()` to measure the distance between neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e18f5b",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "w_dist1 <- nbdists(w_dist, coords) \n",
    "w_distk <- nbdists(w_k, coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66173ad",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "Using `lapply()`, create the inverse distance, and then convert to the weights matrix using `nb2listw()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9061a1a",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dists <- lapply(w_dist1, function(x) 1/(x))\n",
    "w_inv <- nb2listw(w_dist, glist=dists, \n",
    "            style = \"W\", zero.policy = TRUE)\n",
    "\n",
    "distsk10 <- lapply(w_distk, function(x) 1/(x))\n",
    "w_invk10 <- nb2listw(w_k, glist=distsk10, \n",
    "            style = \"W\", zero.policy = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6463a80d",
   "metadata": {},
   "source": [
    "## Spatial Autocorrelation Measures\n",
    "\n",
    "Now that we have created our spatial weights, we can put them to use measuring spatial relationships. In the example below, I use Moran's I, but you could also use Geary's *c* and Getis-Ord *G* statistics. See [here](https://r-spatial.org/book/15-Measures.html#global-measures) for instructions for these statistics. \n",
    "\n",
    "Below, calculate Moran's I using each of our spatial weight matrices. Which spatial weight shows the strongest spatial autocorrelation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7b4627",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "moran.test(df.wide$entropy, listw = w_q, \n",
    "           alternative = \"two.sided\", \n",
    "           zero.policy = TRUE, \n",
    "           na.action = na.omit)\n",
    "\n",
    "moran.test(df.wide$entropy, listw = w_inv, \n",
    "           alternative = \"two.sided\", \n",
    "           zero.policy = TRUE, \n",
    "           na.action = na.omit)\n",
    "    \n",
    "moran.test(df.wide$entropy, listw = w_invk10, \n",
    "           alternative = \"two.sided\",\n",
    "           zero.policy = TRUE, \n",
    "           na.action = na.omit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25970bd6",
   "metadata": {},
   "source": [
    "Local measures of Moran's I can be created using `localmoran()`. Again, we use all three definitions of weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4c0755",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "localm_q <- localmoran(df.wide$entropy,listw = w_q)\n",
    "localm_inv <- localmoran(df.wide$entropy,listw = w_inv)\n",
    "localm_invk10 <- localmoran(df.wide$entropy,listw = w_invk10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c92672",
   "metadata": {},
   "source": [
    "After storing these results, we can create new variables on our spatial dataframe that records the local Moran values that are \"significant\" (Pebesma and Bivand prefer the term \"interesting\"). Below we use significance thresholds of .01 and adjust the p-values for false discovery rates, according to guidance from Caldas de Castro and Singer (2006) using the `hotspot()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6dc3dc",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "df.wide$hotspot_I_q <- hotspot(localm_q, \n",
    "            Prname = \"Pr(z != E(Ii))\",       # this is the name of the pvalue in the table produced by localmoran()\n",
    "            cutoff = 0.01, \n",
    "            p.adjust = \"fdr\")\n",
    "\n",
    "df.wide$hotspot_I_inv <- hotspot(localm_inv, \n",
    "            Prname = \"Pr(z != E(Ii))\", \n",
    "            cutoff = 0.01, \n",
    "            p.adjust = \"fdr\")\n",
    "            \n",
    "df.wide$hotspot_I_invk10 <- hotspot(localm_invk10, \n",
    "            Prname = \"Pr(z != E(Ii))\", \n",
    "            cutoff = 0.01, \n",
    "            p.adjust = \"fdr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568cd02c",
   "metadata": {},
   "source": [
    "Let's graph them and exam the differences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662f38a7",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ggplot(df.wide, aes(fill = hotspot_I_q)) + \n",
    "    geom_sf(linewidth = 0.1) + \n",
    "    theme_void() + scale_fill_viridis_d()\n",
    "\n",
    "ggplot(df.wide, aes(fill = hotspot_I_inv)) + \n",
    "    geom_sf(linewidth = 0.1) + \n",
    "    theme_void() + scale_fill_viridis_d()\n",
    "\n",
    "ggplot(df.wide, aes(fill = hotspot_I_invk10)) + \n",
    "    geom_sf(linewidth = 0.1) + \n",
    "    theme_void() + scale_fill_viridis_d()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90840fa",
   "metadata": {},
   "source": [
    "Clearly, the more expansive view of neighbor (the 20km weights) gives us a larger number of clusters identified. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914a0523",
   "metadata": {},
   "source": [
    "## Spatial Regression Models\n",
    "\n",
    "A series of functions from **{spatialreg}** can be used to estimate spatial models. They are easy to fit. We use the fixed model for each by storing the forumla as `my.formula`. Here, we predict entropy scores by vacancy, median age, and population density. Then, we alter the spatial model by including lagged outcomes, lagged predictors, both, and lags in the error term. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d39735",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# store the base model for simplicity\n",
    "my.formula <- formula(entropy ~ vacancy.renter + median.age + popden)\n",
    "\n",
    "# OLS\n",
    "m.lm <- lm(my.formula, data = df.wide)\n",
    "\n",
    "# Spatial lag of x model: includes lags of Xs but not Y\n",
    "m.slx <- lmSLX(my.formula, data = df.wide, listw = w_q)\n",
    "\n",
    "# Spatial Lag Model or SAR; includes lag of outcome\n",
    "m.slm <- lagsarlm(my.formula, data = df.wide, listw = w_q)\n",
    "\n",
    "# Spatial Durbin model; includes WY lag and two WX covariates\n",
    "m.sd1 <- lagsarlm(my.formula, data = df.wide, listw = w_q, Durbin = ~ vacancy.renter + median.age)\n",
    "\n",
    "# This Durbin model includes all three WXs\n",
    "m.sd2 <- lagsarlm(my.formula, data = df.wide, listw = w_q, Durbin = TRUE)\n",
    "\n",
    "# Spatial Error Model; errors adjusted for spatial autocorrelation of Y\n",
    "m.sem <- errorsarlm(my.formula, data = df.wide, listw = w_q)\n",
    "\n",
    "# Spatial Durbin Error Model; incorporates WXs with spatial error term\n",
    "m.sdem <- errorsarlm(my.formula, data = df.wide, listw = w_q, Durbin = TRUE)\n",
    "\n",
    "# Spatial Autoregressive Combined model; uses both lag and error term of Y\n",
    "m.sac <- sacsarlm(my.formula, data = df.wide, listw = w_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289674eb",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "modelsummary(list(m.lm, m.slm, m.sac, m.slx, m.sd1, m.sd2, m.sdem),\n",
    "             stars = TRUE,\n",
    "             estimate = \"{estimate}{stars}\",\n",
    "             statistic = \"({std.error})\",                  \n",
    "             gof_map = c(\"nobs\", \"r.squared\", \"rmse\", \"aic\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4796c0a",
   "metadata": {},
   "source": [
    "One final step is to examine the direct and indirect effects of your X variables. We can do that with the `impacts()` function, although it requires a little bit of awkward processing first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d957b8",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "W <- as(w_q, \"CsparseMatrix\")\n",
    "trw_q <- trW(W, type=\"mult\")\n",
    "\n",
    "impacts(m.slx)\n",
    "impacts(m.sd2, tr = trw_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454f5b7b",
   "metadata": {},
   "source": [
    "For which X is the largest portion of the effect coming indirectly (that is, from neighbors' values?). Speculate a bit. Why do you think that neighbor's levels of that variable would impact the racial segregation of a tract?  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
