{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "MLE is an estimator commonly used in regression models. MLE asks a simple question: what parameter values for the model are most likely given the data? Or, put differently, what model parameters are most likely to be the cause of data we observe? So MLE works backward, using optimization algorithms to identify the likelihood of the data based on candidate values of our model parameters (like the y-intercept and predictor variable slopes). MLE is particularly useful when we cannot algebraically solve for the parameter values like we can for OLS. Mathematically, it is easier to: \n",
    "\n",
    "    1. Use the log of the likelihood function;\n",
    "    2. Take the minimum of the negative likelihood function\n",
    "\n",
    "So that is why MLE examines negative log-likehoods. The resulting model parameter estimates are the slopes that are most likely to have given rise to the data we observe and are identified at the minimum of the negative log-likelihood function.\n",
    "\n",
    "Let's do a quick exercise using a linear model to illustrate what MLE is doing. I will ***never*** ask you to do this on your own for an exam or project, but it might be help to better understand the class of models which use MLE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# install packages (if needed)\n",
    "# install.packages(\"tidyverse\")\n",
    "# install.packages(\"bbmle\") # bbmle will let us program mle\n",
    "# install.packages(\"marginaleffects\") # interpret results of logit models\n",
    "# install.packages(\"performance\") # for percent correctly predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the packages and create some fake data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(bbmle)\n",
    "library(marginaleffects)\n",
    "library(performance)\n",
    "\n",
    "# set the start point for random numbers so we get the same data\n",
    "set.seed(1234)\n",
    "\n",
    "# generate our error term\n",
    "e <- rnorm(1000, 0, 10) # mean of 0 and sd of 10\n",
    "\n",
    "# generate our predictor variable\n",
    "x <- runif(1000, 18, 98)\n",
    "\n",
    "# generate y as a function of x and error.\n",
    "# true values: b0 = 20, b1 = .8\n",
    "y <- 20 + .8*x + e\n",
    "\n",
    "# combine fake data into tibble dataframe\n",
    "df1 <- tibble(y, x, e)\n",
    "\n",
    "# plot\n",
    "ggplot(df1, aes(x = x, y=y)) + geom_point()\n",
    "\n",
    "# check to make sure OLS gives us correct info\n",
    "ols1 <- lm(y ~ x, data = df1)\n",
    "summary(ols1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's program some MLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create two functions - these specify parameters that \n",
    "# can then take on specific values when you use the \n",
    "# function.\n",
    "\n",
    "# here, we specify the model, working backward from the residuals\n",
    "# our three parameters are the y-intercept, the slope of x\n",
    "# and the standard deviation of the residuals \n",
    "LL <- function(beta0, beta1, sigma){\n",
    "  Resids = y  - x*beta1 - beta0\n",
    "  Resids = suppressWarnings(dnorm(Resids, 0, sigma, log = TRUE))\n",
    "  -sum(Resids)\n",
    "}\n",
    "\n",
    "# use mle2 from the bbmle package to use optimization methods\n",
    "# to find values of parameters. \n",
    "fit <- mle2(minuslogl = LL, # our function\n",
    "            start = list(beta0 = 0, beta1 = 0, sigma = 1) # starting values for parameter search\n",
    "            )\n",
    "fit # call up results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did we do? Did MLE get close to the OLS estimates? What about the original true values? Please answer using a Markdown section of your Quarto submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all fine and good, I guess. But it still is a little bit of a black box. This time, let's fix the parameter values for the standard deviation of the residuals and the y-intercept to their true values, and then graph what happens to the negative log likelihood as we cycle through values for `beta1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# now only beta1 remains as a parameter. beta0 is set to 20 and sigma to 10.\n",
    "LL2 <- function(beta1){\n",
    "  Resids = y  - x*beta1 - 20\n",
    "  Resids = suppressWarnings(dnorm(Resids, 0, 10, log = TRUE))\n",
    "  -sum(Resids)\n",
    "}\n",
    "\n",
    "# create vector of integers from 0 to 200\n",
    "testpars <- 0:200\n",
    "# divide by 100 to scale between 0 and 2\n",
    "testpars <- testpars / 100\n",
    "\n",
    "# take each value in `testpars` and plug into LL2 function \n",
    "LL.out <- lapply(testpars, LL2)\n",
    "# write result of function to dataframe named `dfLL`\n",
    "dfLL <- data.frame(do.call(rbind, LL.out), testpars)\n",
    "\n",
    "# graph result\n",
    "ggplot(data = dfLL, aes(x = testpars, y = do.call.rbind..LL.out.)) +\n",
    "    geom_line() + \n",
    "    theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to drive home the point here, remake the above graph, this time adding in a vertical dashed red line at `x = 0.8`. Add appropriate axis titles to the graph. Change the color of the line and increase the thickness of the line. You might need to review the arguments for `geom_line` in `ggplot()`. You can find out more [here](https://ggplot2.tidyverse.org/reference/geom_path.html). Do you see how MLE uses the minimum negative log likelihood to identify parameter estimates?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is not the only model for analyzing binary outcome variables, but it is the most common (the other very common option is probit regression). Running a logit model is not particularly difficult. You can use `glm()` (stands for *generalized linear model*), where we specify our model of the outcome and the distributional form that we assume the error term takes. This is a necessary part of any MLE approach to modeling. The exact form the linear model takes as it is mapped to the outcome comes from the *link function*. In logistic regression, the error term is from the *binomial distribution* and the link function is the *logit*.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# load packages for interpreting logit models\n",
    "\n",
    "library(marginaleffects) # for interpretation\n",
    "library(performance) # for percent correctly predicted\n",
    "\n",
    "# create simulated data\n",
    "x1 <- rbinom(n = 700, size = 1, prob = 0.5) # dichotomous x variabl\n",
    "x2 <- round(rnorm(700, 6, 2), digits = 0) # normal x variable\n",
    "x3 <- runif(700, 0, 100) # uniform x variable\n",
    "\n",
    "xb <- -2 - 2.6*x1 + .5*x2 + .011*x3 # latent linear variable\n",
    "\n",
    "probs <- 1/(1 + exp(-xb)) # inverse logit transformation \n",
    "\n",
    "# binary outcome y for 700 trials, with the probability y = 1\n",
    "# set by `probs` equation in previous line\n",
    "y <- rbinom(n = 700, size = 1, prob = probs)  \n",
    "\n",
    "# combine vectors into data frame\n",
    "bin.out <- tibble(x1,x2,x3,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can estimate our model using `glm()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# estimate model\n",
    "mod_logit <- glm(y ~ x1 + x2 + x3, \n",
    "                    family = binomial(link = \"logit\"),\n",
    "                    data = bin.out)\n",
    "\n",
    "#check model results\n",
    "summary(mod_logit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One nice statistic to check is the PCP, or the percent correctly predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "performance_pcp(mod_logit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output shows us that our model correctly predicts the value of $y$ in 69% of the observations. The \"null\" model refers to the percent correctly predicted without using any predictor variables. Clearly, knowing our $x$ variables helps us better predict $y$. The likelihood-ratio test is a significant test. The results above show that the difference in PCP between our model and the null is statistically significant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit Postestimation and Interpretation\n",
    "\n",
    "As we have discussed, interpreting logit models is difficult because of the non-linear nature of the inverse logistic curve and the mapping of continuous unbounded values into the probability scale. Typically, we want to know how a predictor variable is expected to change the probability of our units having a characteristic or experiencing an event. In other words, we want to know how $x$ changes $Pr(y = 1)$. We can convey this information in several ways, which have their advantages and disadvantages.\n",
    "\n",
    "### Predicted Probabilities\n",
    "\n",
    "**Predicted probabilities** express the $Pr(y = 1)$ for values of our predictor variable. Let's use predictions from our model to understand our model results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# to generate a predicted probability y = 1 for each observation in your\n",
    "# actual data, use predictions() from the marginaleffects package\n",
    "\n",
    "# syntax: dataframe <- predictions(model name)\n",
    "preds <- predictions(mod_logit) \n",
    "\n",
    "# default is a probability for binary outcome models, stored in `estimate`\n",
    "# notice that we also receive uncertainty estimates\n",
    "head(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, we want these predictions to be graphed by values of our predictor variables. That is, we want to show what happens to $Pr(y = 1)$ by values of $x$. We can do that using `predictions()` as well using further arguments in the function. Since we have two other variables ($x1$ and $x2$) which also determine the $Pr(y = 1)$, we need to fix them to specific values, otherwise we will not receive just one value of $Pr(y = 1)$ for each value of $x3$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "preds <- predictions(mod_logit,\n",
    "\t\t\t\t\t\tcondition = \"x3\", # becomes x axis\n",
    "\t\t\t\t\t\tnewdata = datagrid(x1 = 0.5,     # fix x1 to .5\n",
    "\t\t\t\t\t\t\t\t\t\t   x2 = mean(x2),# fix x2 to its mean\n",
    "\t\t\t\t\t\t\t\t\t\t   x3 = 0:100))  # create table where x3 ranges\n",
    "\t\t\t\t\t\t\t\t\t\t   \t\t\t\t # from 0 to 100\n",
    "\n",
    "# check results\n",
    "head(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! Note that our `preds` dataframe now contains observations with fixed values for $x1$ and $x2$, while $x3$ increases from 0 by 1 unit each observation. If you recall, we created $x3$ from a uniform distribution ranging from 0 to 100, so this reflects the entire range of values of the variable. Now, try your hand at graphing the predicted probability of $y$ by $x3$ using `ggplot()`. Remember to use your predicted dataframe `preds` as the data in `ggplot`. Make the graph look professional, and include the confidence interval around the prediction using the `conf.low` and `conf.high` variables and `ggplot()`'s `geom_ribbon` function. Check out the [geom description here if needed](https://ggplot2.tidyverse.org/reference/geom_ribbon.html?q=geom_ribbon#null). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, `marginaleffects` does have a way to create predicted probabilities and graph them in one function (although you perhaps lose a bit of control of the graphing process)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "p1 <- plot_predictions(mod_logit,\n",
    "\t\t\t\t\t\tcondition = \"x3\", # becomes x axis\n",
    "\t\t\t\t\t\tnewdata = datagrid(x1 = 0.5,\n",
    "\t\t\t\t\t\t\t\t\t\t   x2 = mean(x2),\n",
    "\t\t\t\t\t\t\t\t\t\t   x3 = 0:100))\n",
    "\n",
    "# this graph works with ggplot and we can add other layers\n",
    "p1 + theme_classic() + geom_rug(data = bin.out, aes(x3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also plot predicted probabilities by multiple variables. In the example below, we graph the predicted probability of $y$ by both $x3$ and $x1$. If you recall, $x1$ is a binary predictor variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# what if we want to show two vars?\n",
    "p2 <- plot_predictions(mod_logit,\n",
    "\t\t\t\t\t\tcondition = list(\"x3\", \"x1\"), # becomes x axis\n",
    "\t\t\t\t\t\tnewdata = datagrid(x1 = c(0, 1), # need to specfy both values of x1\n",
    "\t\t\t\t\t\t\t\t\t\t   x2 = mean(x2), # fix x2\n",
    "\t\t\t\t\t\t\t\t\t\t   x3 = 0:100)) # integers for range of x3\n",
    "\n",
    "p2 + theme_minimal() + geom_rug(data = bin.out, aes(x3))\n",
    "\n",
    "# if you'd like to see the specfic predicted values, run same function call \n",
    "# using predictions() instead of plot_predictions()\n",
    "preds <- predictions(mod_logit, newdata = datagrid(x1 = c(0, 1 ),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tx2 = mean(x2),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tx3 = round(min(x3),0):round(max(x3),0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marginal Effects\n",
    "\n",
    "**Marginal effects** is a way of talking about the average change in $Pr(y = 1)$ for a small change in $x$. We typically plot marginal effects over the full range of a predictor variable for models in which the estimated effect changes, like in the logit model or in a model that uses interaction terms. \n",
    "\n",
    "We can use the `slopes` function to generate marginal effects and store as a dataframe or use `plot_slopes` to graph directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# marginal effects\n",
    "p3 <- plot_slopes(mod_logit, \n",
    "\t\t\t\t\t\tvariables = \"x3\", # this is the variable you'll use to\n",
    "\t\t\t\t\t\t\t\t\t\t  # calculate marginal effects \n",
    "\t\t\t\t\t\tcondition = \"x3\", # the x axis on the graph\n",
    "\t\t\t\t\t\tnewdata = datagrid(x1 = 0.5, # fix x1\n",
    "\t\t\t\t\t\t\t\t\t\t   x2 = mean(x2), # fix x2\n",
    "\t\t\t\t\t\t\t\t\t\t   x3 = 0:100)) # values of x3\n",
    "\n",
    "p3\n",
    "\n",
    "  \n",
    "# now with multiple variables\n",
    "p4 <- plot_slopes(mod_logit, \n",
    "\t\t\t\t\t\t\tvariables = \"x3\",\n",
    "\t\t\t\t\t\t\tcondition = list(\"x3\", \"x1\"), # plot marginal effects\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t  # of x3 by values of x3\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t  # and values of x1\n",
    "\t\t\t\t\t\t\tnewdata = datagrid(x1 = c(0,1),\n",
    "\t\t\t\t\t\t\t\t\t\t\t   x2 = mean(x2),\n",
    "\t\t\t\t\t\t\t\t\t\t\t   x3 = 0:100))\n",
    "\n",
    "p4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do you think the effect of $x3$ is going down as $x3$ increases when $x1 = 0$ but going up when $x1 = 1$? After all, the variables are not interacted in the model. Give your explanation in your Quarto report. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First-Differences\n",
    "\n",
    "So-called **first-differences** present changes in the $Pr(y = 1)$ over standardized unit changes of $x$. Typically, this some number of standard deviations below the mean to the same number of standard deviations above the mean on $x$. Another common first difference is a minimum to maximum change. Then, the first difference for one variable could be compared to a first difference for another. First differences are also useful because we specify substantively meaningful changes in $x$ and see if the predicted change in $Pr(y = 1)$ differs significantly from 0. The code below offers examples of some first difference comparisons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# first difference of 1 sd below the mean to 1 sd above the mean\n",
    "# which results in a 2sd change total\n",
    "\n",
    "# setting other xs to means or modes\n",
    "avg_comparisons(mod_logit, \n",
    "\t\t\t\t\t\tvariables = list(x3 = \"2sd\"),\n",
    "\t\t\t\t\t\tnewdata = datagrid())\n",
    "\n",
    "# setting other xs to specific values\n",
    "avg_comparisons(mod_logit, \n",
    "\t\t\t\tvariables = list(x3 = \"2sd\"),\n",
    "\t\t\t\tnewdata = datagrid(x1 = 0.5))\n",
    "\n",
    "# over all values of x1\n",
    "avg_comparisons(mod_logit, \n",
    "\t\t\t\t\tvariables = list(x3 = \"2sd\"),\n",
    "\t\t\t\t\tby = \"x1\",\n",
    "\t\t\t\t\tnewdata = datagrid(x1 = c(0,1)))\n",
    "\n",
    "# test whether first differences of x3 are statistically \n",
    "# different at different values of x1 (you probably would\n",
    "# only do this if you were interacting x1 and x3) \n",
    "avg_comparisons(mod_logit, \n",
    "\t\t\t\t\tvariables = list(x3 = \"2sd\"),\n",
    "\t\t\t\t\tby = \"x1\",\n",
    "\t\t\t\t\thypothesis = \"b1 - b2 = 0\",\n",
    "\t\t\t\t\tnewdata = datagrid(x1 = c(0,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap-up\n",
    "\n",
    "Please let me know if you have any questions about the functions we covered in activity and submit your pdf document rendered from Quarto to Canvas. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
