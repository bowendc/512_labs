{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Grouped or Clustered Data with Multilevel Models\n",
    "\n",
    "We encounter clustered data all the time: survey respondents clustered by city or county, traffic data clustered by ward, developments clustered by zoning regulation, reports clustered by agency, tests clustered by student or patient, and test scores clustered by school and district. Most of the data we use, in fact, could be conceptuatlized as being derived from a *multilevel data generating process (DGP)*. OLS assumes our observations are independent of one another and result in errors that are *i.i.d.* Clustered data violate such assumptions. So how to model clustered data in R?\n",
    "\n",
    "For this activity, we will examine precinct-level results from OH's 2023 [Proposition 1 which codified the right to an abortion](https://ballotpedia.org/Ohio_Issue_1,_Right_to_Make_Reproductive_Decisions_Including_Abortion_Initiative_(2023)) into the state's constitution. We will rely on two data sources: The Ohio Secretary of State's [Election Data and Results](https://www.ohiosos.gov/elections/election-results-and-data/) page (for election results) and the county-level demographic data from the 2020 decennial census from the U.S. Census Bureau. \n",
    "\n",
    "For this activity, please complete all code chunks below in Quarto and render as a pdf for submission. You can skip the `install.packages()` code chunk in Quarto (but you might need to install a package if you have not previously done so). Be sure to complete any requested tasks as you work through the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install any needed packages. We'll be using `lme4` and `estimatr`, which might be new to you. `lme4` will be used to run RE models, while `estimatr` offers a simple interface for specifying FE models and clustering standard errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# install any of the packages below that you need\n",
    "\n",
    "# install.packages('tidyverse')\n",
    "# install.packages('lme4')\n",
    "# install.packages('estimatr')\n",
    "# install.packages('marginaleffects')\n",
    "# install.packages('modelsummary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "for(pack in c('tidyverse', 'lme4', 'estimatr', 'marginaleffects','modelsummary')){\n",
    "    library(pack, character.only = TRUE)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# get prop 1 election data and create outcome measured as proportion\n",
    "oh <- read.csv(\"https://raw.githubusercontent.com/bowendc/512_labs/refs/heads/main/oh_2023.csv\") |>\n",
    "            mutate(yes.pct = 100 * Prop1.Yes / (Prop1.Yes + Prop1.No))\n",
    "\n",
    "# get oh gov election datat from 2022 and create % variable\n",
    "ohgov <- read.csv(\"https://raw.githubusercontent.com/bowendc/512_labs/refs/heads/main/oh_gov_2022.csv\") |>\n",
    "            mutate(demgov.pct  = 100 * dem22gov / (dem22gov + rep22gov),\n",
    "                   cnt.demgov = demgov.pct - mean(demgov.pct, na.rm = TRUE))\n",
    "\n",
    "# get census data \n",
    "county <- read.csv(\"https://raw.githubusercontent.com/bowendc/512_labs/refs/heads/main/oh_county.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine each dataframe. You can use `View(df)` to look at the raw data. You do not need to include those functions in your Quarto document. Using a Markdown chunk, describe the unit of analysis for each dataframe.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, we need to process `county` to get it down to the county level. We can use `pivot_wider` from the `dplyr` package (part of the `tidyerse`) to transpose the dataframe. We just need to describe where the data values come from and where the names come from. Technically, we're moving from a \"long\" dataframe to a \"wide\" dataframe. That is, we're going to express information once stored in **rows** and move them into new **columns**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "county.wide <- county |>\n",
    "            pivot_wider(id_cols = c(GEOID, NAME), # identifies the unit\n",
    "                names_from = variable,   # where the new var names come from\n",
    "                values_from = value)     # source of data values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check to make sure our transposition worked the way we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "head(county.wide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we have county data that we can combine with our precinct data -> two separate levels of data where one is nested within the other. For our own ease of analysis, we can combine these dataframes together. Let's check our other two dataframes again and identify the county variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "head(oh, n = 3)\n",
    "head(ohgov, n = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh oh. Looks like in `oh` and `ohgov` (our precinct data), the county is stored in `County.Name` variable and includes only the name. In `county_wide`, the variable `NAME` includes the name followed by \"County, Ohio\". We need to either remove \"County, Ohio\" from `county_wide` or add it to the other two dataframes. Fortunately, we can accomplish this using string functions that allow us to manipulate text data.\n",
    "\n",
    "Let's add a new variable, titled the same as the county variable is titled in the `oh` and `ohgov` dataframes for convenience later. This new variable will remove the unnecessary text behind the county name. We are fortunate because each county name has the same character string behind it, so it will be easy to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# gsub function looks for a pattern (our text we want changed)\n",
    "# and then replaces it with a new pattern (here I use \"\" with\n",
    "# no space between the quotation marks. That will replace \n",
    "# the pattern with no other text.). The last arg specifies \n",
    "# the vector of data. Here, it is the variable `NAME`. \n",
    "\n",
    "county.wide <- county.wide |> \n",
    "                mutate(County.Name = gsub(\" County, Ohio\", \"\", NAME))\n",
    "\n",
    "head(county.wide$County.Name, n = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to combine dataframes. We can use functions from `tidyverse`'s `dpylr` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# left_join will combine two dataframes based on the variables listed in\n",
    "# by(). Here we use the county name and the precinct code to do our matches\n",
    "# Use codes instead of names when possible. \n",
    "\n",
    "# left_join will keep only unmatched observations from the first df you list\n",
    "# if the observation is unmatched but included in the second df, it will\n",
    "# be dropped from the combined df.\n",
    "oh.comb <- left_join(oh, ohgov, by = c(\"County.Name\", \"Precinct.Code\"))\n",
    "\n",
    "nrow(oh)\n",
    "nrow(ohgov)\n",
    "nrow(oh.comb)\n",
    "\n",
    "# it is ALWAYS a good idea to inspect your dataframe after merging!\n",
    "head(oh.comb)\n",
    "\n",
    "# let's run left_join again and merge in county data\n",
    "oh.comb <- left_join(oh.comb, county.wide, by = \"County.Name\")\n",
    "\n",
    "nrow(oh.comb)\n",
    "head(oh.comb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that every precinct in a county recevies the same values from the `county.wide` dataframe - they are represented in the last 5 variables in the combined dataframe. \n",
    "\n",
    "Ok, now we are almost ready to analyze our multilevel data. Let's create a new variable to record if an abortion provider facility is located in the county. In 2023, facilities providing abortions existed in six OH counties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# create list storing the names of counties with abortion facilities\n",
    "countylist <- c(\"Cuyahoga\", \"Hamilton\", \"Franklin\", \n",
    "           \"Summit\", \"Montgomery\", \"Lucas\")\n",
    "\n",
    "# the ifelse() statement below will be coded 1 if an observation's \n",
    "# County.Name is found in list `countylist` and 0 otherwise\n",
    "oh.comb <- oh.comb |> \n",
    "            mutate(access = ifelse(County.Name %in% countylist, 1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Standard Errors\n",
    "\n",
    "The simplest way to address some of the issues of clustered data is to adjust coefficient standard errors to allow for cluster-correlated error terms. Of course, this approach handles our lack of independent observations but not address un-modeled group heterogeneity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's fit a plain old OLS model predicting support for Prop 1 by the percent of the precinct voting for the Democratic gubernatorial candidate in the previous election. Then, we'll use `lm_robust()` from the `estimatr` package to estimate the OLS model with group-clustered standard errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "m.ols <- lm(yes.pct ~ demgov.pct  + log(totpop) + access +\n",
    "                        pct.black + pct.latino, \n",
    "                        data = oh.comb)\n",
    "\n",
    "m.clse <- lm_robust(yes.pct ~ demgov.pct + log(totpop) + access +\n",
    "                        pct.black + pct.latino, \n",
    "                        cluster = County.Name,     # tells R to cluster SE by county var\n",
    "                        se_type = 'stata',         # standard formula for clustering\n",
    "                        data = oh.comb)\n",
    "\n",
    "# this is a nice way of formating a regression table. It will set the names \n",
    "# we want the coefficients to have in our table. Store as a list.\n",
    "\n",
    "coefmap <- c(\"(Intercept)\" = \"Constant\",\n",
    "            \"demgov.pct\" = \"Precinct Vote for Dem Gov\",\n",
    "            \"log(totpop)\" = \"County Population (logged)\",\n",
    "            \"access\" = \"Abortion Facility in County\",  \n",
    "            \"pct.black\" = \"Proportion Black in County\",\n",
    "            \"pct.latino\" = \"Proportion Latino/a in County\")\n",
    "\n",
    "# use modelsummary to rename models \n",
    "modelsummary(list(\"OLS\" = m.ols, \"OLS with Clustered SEs\" = m.clse),\n",
    "    stars = TRUE,\n",
    "    estimate = \"{estimate}{stars}\",\n",
    "    statistic = \"({std.error})\",\n",
    "    coef_map = coefmap,                       # calls up our stored list\n",
    "    gof_map = c(\"nobs\", \"r.squared\", \"rmse\")) # identifies just our needed stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Markdown, describe the differences between the regular OLS estimates and the OLS estimates with clustered standard errors by county. What types of estimates are impacted, and what difference does the clustering make for how we interpret the regression results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed Effects (FE)\n",
    "\n",
    "Fixed effects models take a different approach. Instead of relaxing the assumption of independent errors, FE models try to model the inter-relatedness of our observations by fitting a dummy variable (or demeaning the data) for each group. So FEs \"soak up\" the variation in $y$ coming from the group-level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# here, we use lm_robust. First, create FE by including County.Name\n",
    "# this works because it is a factor variable. R will create\n",
    "# dummy vars for each county and include in model\n",
    "\n",
    "m.fe <- lm_robust(yes.pct ~ demgov.pct + log(totpop) + \n",
    "                        access + pct.black + pct.latino + factor(County.Name),\n",
    "                        data = oh.comb)\n",
    "\n",
    "# you can see the dummies if you run: \n",
    "summary(m.fe)\n",
    "\n",
    "\n",
    "# if we use the `fixed_effects` estimator directly, it will \n",
    "# \"demean\" the data: meaning, it will subtract the mean value \n",
    "# of y for the group, removing any county-level average \n",
    "# differences. The only remaining variation is coming from within\n",
    "# the counties (our precincts), not across them\n",
    "m.fe.within.panel.est <- m.fe.clse <- lm_robust(yes.pct ~ demgov.pct + log(totpop) + \n",
    "                        access + pct.black + pct.latino,\n",
    "                        fixed_effects = ~County.Name,\n",
    "                        data = oh.comb)\n",
    "\n",
    "# we could also still cluster the SEs, but this is often not\n",
    "# necessary\n",
    "m.fe.clse <- lm_robust(yes.pct ~ demgov.pct + log(totpop) + \n",
    "                        access + pct.black + pct.latino,\n",
    "                        fixed_effects = ~County.Name, \n",
    "                        cluster = County.Name,\n",
    "                        se_type = 'stata',\n",
    "                        data = oh.comb)\n",
    "\n",
    "modelsummary(list(\"FE\" = m.fe, \n",
    "                  \"FE within-panel estimator\" = m.fe.within.panel.est, \n",
    "                  \"FE with Clustered SEs\" = m.fe.clse),\n",
    "                stars = TRUE,\n",
    "                estimate = \"{estimate}{stars}\",\n",
    "                statistic = \"({std.error})\",\n",
    "                coef_map = coefmap,\n",
    "                gof_map = c(\"nobs\", \"r.squared\", \"rmse\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm... where did our county-level variables go? The first model (with our included dummy variables) drops `access`, while all county-level variables are droped in the next two models. Say why you think this happened in a Markdown section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Effects (REs)\n",
    "\n",
    "So-called random effects models take a different approach, separating the variation in $y$ into \"fixed\" portion and \"random\" portions which are assumed to be independent of one another. The random portion has an assumed distribution (usually normal) and can be used to describe the unmodeled group-heterogeneity. Both group-level intercepts and slopes can be included as random effects, and we can create point estimates by group from the model results. This method does let us include group-level predictors. Below, we fit random effects models using `lmer` from the `lme4` package. There is a corresponding `glmer` function for generalized linear models as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# fit a random intercept model with no other predictors \n",
    "# besides the intercept (hence the 1)\n",
    "m.re <- lmer(yes.pct ~ 1 + \n",
    "        (1 | County.Name),  # this means random intercept for each level of county\n",
    "        data = oh.comb)\n",
    "\n",
    "# fit full random intercept model including \"fixed\" portion (standard coefficients)\n",
    "m.re.ri <- lmer(yes.pct ~ demgov.pct + log(totpop) + \n",
    "                        access + pct.black + pct.latino +\n",
    "                        (1 | County.Name),\n",
    "                        data = oh.comb)\n",
    "\n",
    "# fit random intercept, random slope model. \n",
    "m.re.rirs <- lmer(yes.pct ~ demgov.pct + log(totpop) + \n",
    "                        access + pct.black + pct.latino +\n",
    "                        (1 + demgov.pct| County.Name), # now, we have intercept and \n",
    "                        data = oh.comb)                # slopes for demgov.pct that \n",
    "                                                       # vary by county\n",
    "\n",
    "modelsummary(list(\"Naive Random Intercept\" = m.re, \n",
    "                  \"Random Intercept\" = m.re.ri, \n",
    "                  \"Random Intercepts and Slope\" = m.re.rirs), \n",
    "                stars = TRUE,\n",
    "                estimate = \"{estimate}{stars}\",\n",
    "                statistic = \"({std.error})\",\n",
    "                gof_map = c(\"nobs\", \"icc\", \"rmse\", \"aic\", \"bic\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we can now get estimates for all county-level variables. Notice also that the coefficient estimate for `demgov.pct` remains very similar to that from the FE models until we let the slope vary by county (final model above). In that model, the average coef is quite a bit larger, although some counties will have lower slopes and some higher. \n",
    "\n",
    "The ICC (intra-class correlation coefficient) shows the proportion of the unexplained variance in $y$ coming from the group-level. In the naive model with no predictors, `lmer` estimates half of the variation in $y$ comes from the county level and half from the precinct level. When we re-estimate with our predictor variables included, we receive the same 0.5 ICC result. However, when we let the slopes of `demgov.pct` vary by county, we are better able to capture the precint-level variation in Prop 1 support. Now 80% of the remaining variation in $y$ comes from the county level.\n",
    "\n",
    "It's a good idea to plot the results of multilevel model if you are interested in the group-level heterogeneity. Let's present both our random intercept and our random-intercept, random-slope models to predict the relationship between precinct-level Democratic voteshare and Prop 1 support. Of note, we need to let this relationship vary by county!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# plot_predictions() is from the marginaleffects package\n",
    "\n",
    "# first for just the random intercept model\n",
    "plot_predictions(m.re.ri, condition = list(\"demgov.pct\", \"County.Name\")) +\n",
    "      theme_minimal() + theme(legend.position = \"none\")\n",
    "\n",
    "# now the random intercept, random slope model\n",
    "plot_predictions(m.re.rirs, condition = list(\"demgov.pct\", \"County.Name\")) +\n",
    "      theme_minimal() + theme(legend.position = \"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is ugly and hard to read. Let's store the predictions and plot using `ggplot()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "out <- plot_predictions(m.re.rirs, condition = list(\"demgov.pct\", \"County.Name\"),\n",
    "                 draw = FALSE) \n",
    "\n",
    "# geom_rug gives those tick marks on the x-axis to show the distribution of values.\n",
    "# sides = \"b\" will include only the rug plot on the x-axis, not the y\n",
    "\n",
    "# note also that I am using the original data, not predicted data, for the rug plot\n",
    "# but am using the predicted data (`out`) for the geom_line \n",
    "\n",
    "ggplot() +\n",
    "    geom_line(data = out, aes(demgov.pct, estimate, group = County.Name),\n",
    "        color = \"black\", alpha = .1) + \n",
    "    geom_rug(data = oh.comb, aes(x = demgov.pct), sides = \"b\", alpha = .025) + \n",
    "    theme_minimal() + theme(legend.position = \"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, the effect of Democratic strength at the precinct level appears to vary by county, with the relationship much stronger in some counties than others. We could use this information to think more deeply about why - maybe we need to account for urbanism, education, relgiosity, or other factors if we want to better understand this differing relationship across places. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
